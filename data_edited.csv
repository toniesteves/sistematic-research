Document Title,Abstract
An efficient FPGA implementation of HEVC intra prediction,"Intra prediction algorithm used in High Efficiency Video Coding (HEVC) standard has very high computational complexity. In this paper, an efficient FPGA implementation of HEVC intra prediction is proposed for 4√ó4, 8√ó8, 16√ó16 and 32√ó32 angular prediction modes. In the proposed FPGA implementation, one intra angular prediction equation is implemented using one DSP block in FPGA. The proposed FPGA implementation, in the worst case, can process 55 Full HD (1920√ó1080) video frames per second. It has up to 34.66% less energy consumption than the original FPGA implementation of HEVC intra prediction. Therefore, it can be used in portable consumer electronics products that require a real-time HEVC encoder."
Full-HD Accelerated and Embedded Feature Detection Video System with 63fps using ORB for FREAK,"Detecting features using pattern recognition algorithms is a fast method to recognize objects without needing large image databases. Feature detection can be used in e.g. machine learning or Simultaneous Localization and Mapping (SLAM) algorithms. For several application areas, real-time requirements and power constraints are important. FPGAs are predestined for these requirements, due to the possibility of clock-accurate computations. This work proposes a hardware accelerator of the Oriented FAST and Rotated BRIEF (ORB) feature detector implemented as a pipelined design on an FPGA SoC. The accelerator is integrated into an HDMI input/output video processing system. The final system runs with a frequency of 148.5 MHz and processes a 1080p video stream with 63 frames per second (fps). Furthermore, we propose to replace the Binary Robust Independent Elementary Features (BRIEF) with the Fast Retina Keypoint (FREAK) descriptor to improve the performance and repeatability of the algorithm."
A Binary-Feature-Based Object Recognition Accelerator with 22 M-Vector/s Throughput and 0.68 G-Vector/J Energy-Efficiency for Full-HD Resolution,"Considering that the binary-feature-based approximate nearest neighbor search technique has not been fully exploited to date, a multi-segment binary feature-based hierarchical clustering tree model is proposed to achieve fast binary feature matching. In addition, the multi-segment vocabulary forest (MSVF), is developed for the ease of hardware-oriented implementation. During the approximate nearest neighbor searching process, the corresponding leaf nodes of each segment of the query feature are returned simultaneously to improve processing speed and accuracy. Furthermore, a hierarchical decomposition based on the term frequency-inverse document frequency is used to reduce the run-time search space and total memory footprint for object database storage. Finally, a fine-grained feature-level fully-pipelined object recognition accelerator is implemented based on a dedicated design between feature matching and object scoring. The performance of the proposed object recognition accelerator is evaluated based on TSMC 65 nm CMOS technology. The accelerator achieves 22 M-vec/s and 6.8&#x00D7;108 vec/J in throughput and energy efficiency for full-HD resolution, respectively; these results represent a 10.6&#x00D7; and 9&#x00D7; improvement, respectively, relative to current state-of-the-art solutions. The average power consumption is 32.6 mW when operating at 200 MHz."
Design Flow of Accelerating Hybrid Extremely Low Bit-Width Neural Network in Embedded FPGA,"Neural network accelerators with low latency and low energy consumption are desirable for edge computing. To create such accelerators, we propose a design flow for accelerating the extremely low bit-width neural network (ELB-NN) in embedded FPGAs with hybrid quantization schemes. This flow covers both network training and FPGA-based network deployment, which facilitates the design space exploration and simplifies the tradeoff between network accuracy and computation efficiency. Using this flow helps hardware designers to deliver a network accelerator in edge devices under strict resource and power constraints. We present the proposed flow by supporting hybrid ELB settings within a neural network. Results show that our design can deliver very high performance peaking at 10.3 TOPS and classify up to 325.3 image/s/watt while running large-scale neural networks for less than 5W using embedded FPGA. To the best of our knowledge, it is the most energy efficient solution in comparison to GPU or other FPGA implementations reported so far in the literature."
DLA: Compiler and FPGA Overlay for Neural Network Inference Acceleration,"Overlays have shown significant promise for field-programmable gate-arrays (FPGAs) as they allow for fast development cycles and remove many of the challenges of the traditional FPGA hardware design flow. However, this often comes with a significant performance burden resulting in very little adoption of overlays for practical applications. In this paper, we tailor an overlay to a specific application domain, and we show how we maintain its full programmability without paying for the performance overhead traditionally associated with overlays. Specifically, we introduce an overlay targeted for deep neural network inference with only ~1% overhead to support the control and reprogramming logic using a lightweight very-long instruction word (VLIW) network. Additionally, we implement a sophisticated domain specific graph compiler that compiles deep learning languages such as Caffe or Tensorflow to easily target our overlay. We show how our graph compiler performs architecture-driven software optimizations to significantly boost performance of both convolutional and recurrent neural networks (CNNs/RNNs) - we demonstrate a 3x improvement on ResNet-101 and a 12x improvement for long short-term memory (LSTM) cells, compared to naive implementations. Finally, we describe how we can tailor our hardware overlay, and use our graph compiler to achieve ~900 fps on GoogLeNet on an Intel Arria 10 1150 - the fastest ever reported on comparable FPGAs."
DNNBuilder: an Automated Tool for Building High-Performance DNN Hardware Accelerators for FPGAs,"Building a high-performance FPGA accelerator for Deep Neural Networks (DNNs) often requires RTL programming, hardware verification, and precise resource allocation, all of which can be time-consuming and challenging to perform even for seasoned FPGA developers. To bridge the gap between fast DNN construction in software (e.g., Caffe, TensorFlow) and slow hardware implementation, we propose DNNBuilder for building high-performance DNN hardware accelerators on FPGAs automatically. Novel techniques are developed to meet the throughput and latency requirements for both cloud- and edge-devices. A number of novel techniques including high-quality RTL neural network components, a fine-grained layer-based pipeline architecture, and a column-based cache scheme are developed to boost throughput, reduce latency, and save FPGA on-chip memory. To address the limited resource challenge, we design an automatic design space exploration tool to generate optimized parallelism guidelines by considering external memory access bandwidth, data reuse behaviors, FPGA resource availability, and DNN complexity. DNNBuilder is demonstrated on four DNNs (Alexnet, ZF, VGG16, and YOLO) on two FPGAs (XC7Z045 and KU115) corresponding to the edge- and cloud-computing, respectively. The fine-grained layer-based pipeline architecture and the column-based cache scheme contribute to 7.7x and 43x reduction of the latency and BRAM utilization compared to conventional designs. We achieve the best performance (up to 5.15x faster) and efficiency (up to 5.88x more efficient) compared to published FPGA-based classification-oriented DNN accelerators for both edge and cloud computing cases. We reach 4218 GOPS for running object detection DNN which is the highest throughput reported to the best of our knowledge. DNNBuilder can provide millisecond-scale real-time performance for processing HD video input and deliver higher efficiency (up to 4.35x) than the GPU-based solutions."
High Throughput and Low Cost Memory Architecture for Full Search Integer Motion Estimation in HEVC,"The two-dimensional matrix array Integer Motion Estimation (IME) architecture using Full Search Motion Estimation (FSME) algorithm for High Efficiency Video Coding (HEVC) is presented in this paper. This architecture can operate for 4K resolution video at 30 fps with latency as low as 1219 clock cycles, allowing the design working in real-time. The memory required also kept as low as 12.5kB. The proposed architecture can reach maximum frequency of 148.6 MHz using 40 nm or 195.4 MHz in 28nm Virtex-6 FPGA using Xilinx ISE version 13.1."
Data representation and hardware aspects in a fully-folded successive-cancellation polar decoder,"In this paper, we introduce a hardware architecture for a polar-code decoder based on Successive Cancellation (SC). SC, proposed by Arikan, achieves the capacity of binary erasure channel. We propose an architecture for a fully-folded SC decoder, of which the basic component is a butterfly. This butterfly performs all computations on the SC data flow graph, while LLRs and partial results are stored in RAMs. Furthermore the impact of LLR quantization and internal data representation on BER performance is studied for codes with block-length of 512, 1024 and 2048 bits. A data representation scheme is proposed and it is shown to lead to 30-50% reduction of required memory, for practical block lengths. The reduction achieved increases with block length."
A Real-Time Convolutional Neural Network for Super-Resolution on FPGA with Applications to 4K UHD 60 fps Video Services,"In this paper, we present a novel hardware-friendly super-resolution (SR) method based on convolutional neural networks (CNN) and its dedicated hardware (HW) on Field Programmable Gate Array (FPGA). Although CNN-based SR methods have shown very promising results for SR, their computational complexities are prohibitive for hardware implementation. To the best of our knowledge, we are the first to implement a real-time CNN-based SR HW that upscales 2K full high-definition (FHD) video to 4K ultra high-definition (UHD) video at 60 frames per second (fps). In our dedicated CNN-based SR HW, low resolution (LR) input frames are processed line-by-line, and the number of convolutional filter parameters is reduced significantly by incorporating depth-wise separable convolutions with a residual connection. Our CNN-based SR HW incorporates a cascade of 1D convolutions having large receptive fields along horizontal lines while keeping vertical receptive fields minimal, which allows to save required line memory space in achieving comparable SR performance against full 2D convolution operations. For efficient HW implementation, we use a simple and effective quantization method with little peak signal-to-noise ratio (PSNR) degradation. Also, we propose a compression method to efficiently store intermediate feature map data to reduce the number of line memories used in HW. Our HW implementation on the FPGA generates 4K UHD frames of higher PSNR values at 60 fps and shows better visual quality, compared to conventional CNN-based SR methods that are trained and tested in software."
Efficient Algorithm Adaptations and Fully-Parallel Hardware Architecture of H.265/HEVC Intra Encoder,"The growing demand for high-performance ultra- high-definition video coding leads to H.265/HEVC, where the increased computational complexity and data/timing dependency hinder its coding throughput. To address these challenges, this paper presents four algorithm adaptations and a fully-parallel hardware architecture for H.265/HEVC intra encoder. To our best knowledge, this is the first fully-parallel H.265/HEVC intra encoder. This design supports 35 prediction modes and all CTU partitions. All PUs are independently processed in four prediction engines for high parallelism. An appropriate set of intra prediction modes, RDO candidates, and CABAC rate estimate instances is assigned to each PE, where internal computational tasks are pipelined and scheduled to maximize the processing throughput. Compared with the HM-15.0 software, the proposed algorithm adaptations lead to a reduction of 27% in computational workload, while the average BD-Rate and BD-PSNR are 4.39% and -0.21dB, respectively. This BD-Rate is lower than existing designs with the same video resolution. FPGA implementation of proposed design shows it operates at 120MHz and supports 45 fps of 1080P video sequences using 201K logic elements and 120KB on-chip SRAM. ASIC implementation of the proposed design in TSMC 90nm technology shows that its clock frequency reaches 320MHz with a hardware gate count of 2288K, and it supports real-time encoding of 30 fps of 4K video sequences. Compared with state-of-the-art designs, our proposed design demonstrates advantages in computational complexity, bit rate, video quality, throughput, reliability, and flexibility."
Design of a Gabor Filter HW Accelerator for Applications in Medical Imaging,"The Gabor filter (GF) has been proved to show good spatial frequency and position selectivity, which makes it a very suitable solution for visual search, object recognition, and, in general, multimedia processing applications. GFs prove useful also in the processing of medical imaging to improve part of the several filtering operations for their enhancement, denoising, and mitigation of artifact issues. However, the good performances of GFs are compensated by a hardware complexity that traduces in a large amount of mapped physical resources. This paper presents three different designs of a GF, showing different tradeoffs between accuracy, area, power, and timing. From the comparative study, it is possible to highlight the strength points of each one and choose the best design. The designs have been targeted to a Xilinx field-programmable gate array (FPGA) platform and synthesized to 90-nm CMOS standard cells. FPGA implementations achieve a maximum operating frequency among the different designs of 179 MHz, while 350 MHz is obtained from CMOS synthesis. Therefore, 86 and 168 full-HD (1920 √ó 1080) f/s could be processed, with FPGA and std_cell implementations, respectively. In order to meet space constraints, several considerations are proposed to achieve an optimization in terms of power consumption, while still ensuring real-time performances."
System Services for Reconfigurable Hardware Acceleration in Mobile Devices,"FPGAs have been deployed to provide custom hardware acceleration for applications such as networking, vision, and machine learning in embedded devices. A mobile device serves increasingly diverse computing applications under constrained resources, hence, making FPGA-based acceleration more attractive to achieve high performance, and yet, energy efficiency. However, due to lack of systematic interface between mobile application software framework and FPGA design flow, software developers avoid using FPGA accelerators in developing Apps for mobile applications. In this paper, we introduce a software framework that integrates the Android operating system with programmable hardware accelerator design flow. We present an acceleration service in Android platform that manages the mapping of compute-intensive kernels to the accelerators on FPGA. The proposed framework enables multiple applications to access multiple FPGA-based accelerators, simultaneously. We present an Android OS porting on the Xilinx Zynq SoC, equipped with dual-core ARM Cortex-A9 processors and programmable logic. The experimental results show performance gain in case studies of canny edge detection algorithm, digit recognition algorithm, and neural network, which are used by mobile Apps."
Low-Power and High-Throughput Architecture for 3D-HEVC Depth Modeling Mode 4,"The 3D-High Efficiency Video Coding (3D-HEVC) is an extension of the High Efficiency Video Coding (HEVC) standard targeting 3D-video encoding. The use of this extension leads to a significant computational-effort increase since new tools are introduced to efficiently encode 3D videos. Therefore, the real-time processing of 3D-videos using 3D-HEVC is highly challenging, and it requires the development of efficient hardware solutions, mainly when mobile devices with energy constraints are considered. This paper focuses on the Depth Modeling Mode 4 (DMM4) encoding mode which is one of the novelties introduced by 3D-HEVC. A low-power and high-throughput architecture was designed and presented in this paper targeting the DMM4, and this architecture was synthesized targeting FPGA and ASIC. The FPGA synthesis was focused on an Altera Stratix V, and the ASIC synthesis was focused on the 45nm Nangate technology. The FPGA and ASIC results showed that the architecture can process UHD 2160p 3D-videos (two views) at 60 fps or to process five views of HD 1080p 3D-videos at 30 frames per second, surpassing the related works in area usage, power dissipation, and processing rate."
Hardware Architecture for Fast General Object Detection using Aggregated Channel Features,"For embedded system applications, high detection accuracy and fast detection must be achieved within a limited power budget. This paper proposes an embedded system-oriented hardware accelerator for object detection with aggregated channel features (ACF). The proposed accelerator consists of hardware architectures dedicated for HOG features, quantization, and boosted decision trees, and they contribute to 2006X speed-up and 601X memory reduction. Our FPGA implementation result shows that the proposed accelerator can detect pedestrians at 170 fps for Full HD images, and 6-class traffic objects at 78 fps for Full HD images."
Multi-precision convolutional neural networks on heterogeneous hardware,"Fully binarised convolutional neural networks (CNNs) deliver very high inference performance using single-bit weights and activations, together with XNOR type operators for the kernel convolutions. Current research shows that full binarisation results in a degradation of accuracy and different approaches to tackle this issue are being investigated such as using more complex models as accuracy reduces. This paper proposes an alternative based on a multi-precision CNN frame-work that combines a binarised and a floating point CNN in a pipeline configuration deployed on heterogeneous hardware. The binarised CNN is mapped onto an FPGA device and used to perform inference over the whole input set while the floating point network is mapped onto a CPU device and performs re-inference only when the classification confidence level is low. A light-weight confidence mechanism enables a flexible trade-off between accuracy and throughput. To demonstrate the concept, we choose a Zynq 7020 device as the hardware target and show that the multi-precision network is able to increase the BNN accuracy from 78.5% to 82.5% and the CPU inference speed from 29.68 to 90.82 images/sec."
Accelerating block-matching and 3D filtering-based image denoising algorithm on FPGAs,"Image denoising is an important pre-processing step in the field of computer vision. And the block-matching and 3D filtering (BM3D) algorithm has achieved quite impressive results in the terms of denoising quality, but it is not being widely used due to its computational complexity. In this paper, we consider FPGA as accelerator because of its reconfigurability and advantage on energy efficiency over GPU. We present a OpenCL-Based FPGA implementation which increased the image processing speed significantly. At the same time, the denoising quality of the image is not affected too much. To our knowledge, this is the first successful attempt of the BM3D algorithm on FPGAs. Also, our implementation is more fine-grained partition of the algorithm, so some parts of our algorithmic, such as block-matching, can be utilized separately to other similar algorithms."
Design and Implementation of 2D IDCT/IDST-Specific Accelerator on Heterogeneous Multicore Architecture,"The paper talks about how to implement different sizes of Inverse Discrete Cosine Transform (IDCT) as well as Inverse Discrete Sine transform (IDST) that are dedicated on High Efficiency Video Coding (HEVC) standard through employing Coarse-Grained Reconfigurable Arrays (CGRAs) as a template-based accelerators on Heterogeneous Accelerator-Rich Platform (HARP). The proposal designs multi-purpose IDCT/IDST-based accelerators in a manner that the final architecture is made up of 4-point IDST and 4/8-point IDCT. The designing of the accelerators is done by creating template-based CGRA devices at various dimensions after which they are arranged in a sequential manner over a structure that is Network-on-Chip (NoC) based accompanied by a number of RISC cores. The research records the IDCT/IDST-specific accelerator performance, the entire platform's performance, as well as the traffic of the NoC with regard to the total number of clock cycles made as well as several other high-level metrics of performance. The experiments that were conducted found that 4-point IDCT and 4-point IDST can be totally implemented in 56 clock cycles. For 8-point IDCT, the clock cycles required are 64. The total power dissipation, as well as energy consumption centred on information on routing and post placement, are all equal to 4.03 mW and 1.76 ŒºJ for 4-point IDCT/IDST and 3.06 ŒºJ for 8-point IDCT, respectively. Furthermore, the use of 256 instantiated Processing Elements (PEs) at an operating frequency of 200.0 MHz results to a 51.2 Giga Operations Per Second (GOPS) performance and 0.012 GOPS/mW architectural constant for the HARP model on the 28 nm Altera Stratix-V chip. The architecture under the proposal is capable of fully sustaining a format of Full HD 1080P at 30 fps on FPGA."
Memory-Centric Flooded LDPC Decoder Architecture Using Non-surjective Finite Alphabet Iterative Decoding,"Non-Surjective Finite Alphabet Iterative Decoding (NS-FAID) represents an LDPC decoding algorithm that uses reduced message storage, with similar or improved error correction performance with respect to Min-Sum. In this paper, we employ NS-FAID compression tables for a memory-centric flooded LDPC decoding architecture. Due to the approximate message storage, improved memory footprint and overall cost is obtained using the NS-FAID approach. We present FPGA synthesis results, in terms of LUT-FF pairs used, working frequency and throughput, as well as Throughput to Area Ratio (TAR). The estimates indicate that employing NS-FAID compression tables yield improvements between 25% and 110% in TAR with respect to the baseline Min-Sum decoder."
Low Power Motion Estimation Algorithm and Architecture of HEVC/H.265 for Consumer Applications,"High-quality videos like high-definition (HD) and ultra HD became an essential requirement in recent applications such as security surveillance, television system, etc. However, due to increase in resolution of the videos, the volume of visual information data increases significantly, which became a challenge for storage, transmission and processing the HD video data. The new video compression standard, high efficient video coding (HEVC), achieved two-fold video efficiency improvements as compared to H.264/AVC using efficient compression techniques. Motion estimation (ME) is one of the computationally intensive blocks in video CODEC. In HEVC, the complexity of ME further increases due to a large processing unit and flexible partitioning of the prediction unit (PU). In this paper, we proposed a low power ME algorithm and architecture of the HEVC for consumer applications. The proposed algorithm and architecture utilizes sub-sampling, data reuse, pixel truncation and adaptive search range techniques for reducing the computational power. Simulations result shows that the proposed ME algorithm requires an average of 53.82% fewer search points as compared to the reference software HM with a small degradation in PSNR and little increment in bit-rate. The proposed architecture is simulated and synthesized using standard 90 nm technology. The proposed ME architecture can process 3840 √ó 2160 @ 30 fps video sequences with only 4.5193 mm<sup>2</sup>of the area and 8.192 KB of SRAM. The operating frequency of the proposed architecture is 250 MHz with 151.7619 mW of power."
VLSI Architecture of High Speed SAD for High Efficiency Video Coding (HEVC) Encoder,This paper presents the architecture of integer motion estimation for HEVC encoder. Our proposed architecture uses pipelined design with parallel processing to increase the performance and reduce computation time. The architecture has 16 processing unit to calculate sum of absolute difference (SAD) of all possible blocks from 4√ó4 to 64√ó64. The architecture has flexibility to adapt any kind of search algorithms including full and fast search. This architecture is prototyped and simulated on Xilinx Virtex-5 FPGA for XC5VLX20T family. Our architecture works on 65 nm technology and attains a maximum clock frequency of 475.21 MHz. The performance of algorithm is compared and has better speed than similar works.
Digital Acquisition Chain for the Upgrade of the CERN SPS Beam Position Monitor,"This paper presents the development of the digital acquisition chain for the upgrade of the Beam Position Monitor (BPM) system for the Super Proton Synchrotron (SPS) at CERN. The front-end electronics will be based on logarithmic amplifiers and, including the digitization, it will be located inside the accelerator tunnel, with optical latency deterministic transmission to surface back-end electronics, where the data stream is processed in a Field Programmable Gate Array. The paper focuses on the processing techniques for the position estimation out of beam data conditioned by logarithmic amplifiers. The elements of the acquisition chain have been tested in laboratory and with beam signals."
A Hardware-Efficient Recognition Accelerator Using Haar-Like Feature and SVM Classifier,"Significantly improved performance of the various learning algorithms has revived the interest in computer vision for recognition applications during the current decade. This paper reports a vision-based hardware recognition architecture combining the Haar-like feature extraction with the support vector machine (SVM) classification. To support an optimal tradeoff between resource requirements, processing speed, and recognition accuracy, a 12-bit fixed-point computation for block-based feature normalization and a recycling allocation of minimalized memory resources are proposed in this paper. Furthermore, an efficient scale generation of target objects for recognition is enabled by configurable windows with high size flexibility. Additionally, a parallel-partial SVM-classification architecture is developed for improving the recognition speed, by accumulating the partially completed SVM results for multiple windows in parallel. The proposed hardware architecture is verified with an Altera DE4 platform to achieve a high throughput rate of 216 and 70 f/s for XGA (1024√ó768) and HD (1920√ó1080) video resolutions, respectively. A recycled memory space of only 193 KB is sufficient for processing high-resolution images up to 2048√ó2048 pixels during online testing. Using the INRIA person dataset, 89.81% average precision and maximum accuracy of 96.93% for pedestrian recognition are realized. Furthermore, about 99.08% accuracy is achieved for two car recognition tasks using the UIUC dataset (side view of cars) and a frontal car dataset collected by ourselves at Hiroshima University with the proposed hardware-architecture framework."
High-throughput HW-SW implementation for MV-HEVC decoder,"The multi-view HEVC extension was completed in July 2014 by the Motion Picture Experts Group and the Video Coding Experts Group. Multi-view video based on stereo representation has become more and more popular. In addition, a variety of multimedia content can now be provided for mobile devices. Therefore, there is a need for a real-time multi-view video decoder. Obviously, because of the complexity of software-based methods, high-resolution MV-HEVC video cannot be decompressed in real time. Especially in mobile devices, there is no hardware that can support MV-HEVC decoding today. Hence, in this paper, we present a new parallel architecture for MV-HEVC and use FPGA to accelerate complex operations. The experimental results show that the proposed multi-view video coding can decompress 3 view video of 1920 √ó 1080 resolution in real time on the ZYNQ platform."
Design and Implementation of Efficient Streaming Deblocking and SAO Filter for HEVC Decoder,"This paper aims to design an efficient mixed serial five-stage pipeline processing hardware architecture of deblocking filter (DBF) and sample adaptive offset (SAO) filter for high efficiency video coding decoder. The proposed hardware is designed to increase the throughput and reduce the number of clock cycles by processing the pixels in a stream of 4 √ó 36 samples in which edge filters are applied vertically in a parallel fashion for processing of luma/chroma samples. Subsequently these filtered pixels are transposed and reprocessed through vertical filter for horizontal filtering in a pipeline fashion. Finally, the filtered block transposed back to the original orientation and forwarded to a three-stage pipeline SAO filter. The proposed architecture is implemented in field programmable gate array and application specific integrated circuit platform using 90-nm library. Experimental results illustrate that the proposed DBF and SAO architecture decreases the processing cycles (172) required for processing each 64 √ó 64 or large coding unit compared with the state-of-the-art literature with the increase of gate count (593.32K) including memory. The results show that the throughput of the proposed filter can successfully decode ultrahigh definition video sequences at 200 frames/s at 341 MHz."
A Neuromorphic Approach to Path Integration: A Head-Direction Spiking Neural Network with Vision-driven Reset,"Simultaneous localization and mapping (SLAM) is one of the core tasks of mobile autonomous robots. Looking for power efficient and embedded solutions for SLAM is an important challenge when building controllers for small and agile robots. Biological neural systems of even simple animals are until now unprecedented in their ability to localize themselves in an unknown environment. Neuromorphic engineering offers ultra low-power and compact computing hardware, in which biologically inspired neuronal architectures for SLAM can be realised. In this paper, we propose an on chip approach for one of the components of SLAM: path integration. Our solution takes inspiration from biology and uses motor command information to estimate the orientation of an agent solely in a spiking neural network. We realise this network on a neuromorphic device that implements artificial neurons and synapses with analog electronics. The neural network receives visual input from an event-based camera and uses this information to correct the on-chip spiking neurons estimate of the robot's orientation. This system can be easily integrated with other localization and mapping components on chip and is a step towards a fully neuromorphic SLAM."
Designing Compact Convolutional Neural Network for Embedded Stereo Vision Systems,"Autonomous systems are used in a wide range of domains from indoor utensils to autonomous robot surgeries and self-driving cars. Stereo vision cameras probably are the most flexible sensing way in these systems since they can extract depth, luminance, color, and shape information. However, stereo vision based applications suffer from huge image sizes and computational complexity leading system to higher power consumption. To tackle these challenges, in the first step, GIMME2 stereo vision system [1] is employed. GIMME2 is a high-throughput and cost efficient FPGA-based stereo vision embedded system. In the next step, we present a framework for designing an optimized Deep Convolutional Neural Network (DCNN) for time constraint applications and/or limited resource budget platforms. Our framework tries to automatically generate a highly robust DCNN architecture for image data receiving from stereo vision cameras. Our proposed framework takes advantage of a multi-objective evolutionary optimization approach to design a near-optimal network architecture for both the accuracy and network size objectives. Unlike recent works aiming to generate a highly accurate network, we also considered the network size parameters to build a highly compact architecture. After designing a robust network, our proposed framework maps generated network on a multi/many core heterogeneous System-on-Chip (SoC). In addition, we have integrated our framework to the GIMME2 processing pipeline such that it can also estimate the distance of detected objects. The generated network by our framework offers up to 24x compression rate while losing only 5% accuracy compare to the best result on the CIFAR-10 dataset."
An HEVC fractional interpolation hardware using memory based constant multiplication,"Fractional interpolation is one of the most computationally intensive parts of High Efficiency Video Coding (HEVC) video encoder and decoder. In this paper, an HEVC fractional interpolation hardware using memory based constant multiplication is proposed. The proposed hardware uses memory based constant multiplication technique for implementing multiplication with constant coefficients. The proposed memory based constant multiplication hardware stores pre-computed products of an input pixel with multiple constant coefficients in memory. Several optimizations are proposed to reduce memory size. The proposed HEVC fractional interpolation hardware, in the worst case, can process 35 quad full HD (3840√ó2160) video frames per second. It has up to 31% less energy consumption than original HEVC fractional interpolation hardware."
Design and Implementation of Multi-Purpose DCT/DST-Specific Accelerator on Heterogeneous Multicore Architecture,"This paper presents the implementation of various sizes of Discrete Cosine transform (DCT) and Discrete Sine Transform (DST) dedicated for High Efficiency Video Coding (HEVC) standard by using template-based Coarse-Grained Reconfigurable Arrays (CGRAs) as accelerators on Heterogeneous Accelerator-Rich Platform (HARP). The proposal makes multipurpose DCT/DST specific accelerators in such a way that final architecture consists of 4/8/16/32-point DCT and 4-point DST. The accelerators are primarily designed by crafting template-based CGRA devices at different dimensions and then arranging them on a Network-on-Chip platform along with a few RISC cores. In this research work, the performance of each DCT/DST-specific accelerator, the collective performance of the whole platform and the NoC traffic are recorded in terms of the number of clock cycles and several high-level performance metrics. Conducted experiments show that 4-point DCT and 4-point DST can be implemented completely in 54 and 56 clock cycles, respectively, while for 8/16/32-point DCT, 67, 179 and 354 clock cycles are required, respectively. The achieved total power dissipation and energy consumption based on post placement and routing information are equal to 4.1 W and $10.87~\mu {\mathrm {J}}$, respectively with 256 instantiated Processing Elements (PEs) at 200.0 MHz operating frequency. It resulted to a performance of 51.2 Giga Operations Per Second (GOPS) and 12 MOPS/mW as an architectural constant for the HARP template on 28 nm Altera Stratix-V chip. The proposed architecture is able to sustain Full HD 1080p format at 30 fps on FPGA."
Register array-based sum of absolute difference processor with parallel memory system for fast motion estimation,"Fast search block matching algorithm (BMA)-based video coding provides reasonable good quality video with minute cost of computation. In fast BMA, clock cycles required to read pixel data are quite more compared with matching operation due to erratic location of candidate macroblocks (CMBs). With aim of reduction in number of clock cycles, parallel memory system is used in this study, which can accelerate reading of CMBs and speedup motion vector (MV) computation. Novel concept of register array is introduced to organise CMBs, which expedite computation hungry search process. Owing to shape of register array, lesser space is needed to store CMBs and architecture addresses wide range of search patterns. The proposed sum of absolute difference processor with parallel memory system computes MV of 1 macroblock in 28 clock cycles in average case. Compared to single memory system, it saves 68% and 80% clock cycles in CMB access of initial search and intermediate search process, respectively. Hardware architecture is tested with Xilinx Virtex5 field programmable gate array. The proposed fixed 8√ó8 macroblock size architecture processes 354 high definition (HD) (1080p) frames per second (fps) and configurable architecture processes 201 HD fps which is more than adequate for real-time encoding."
Approximate HEVC Fractional Interpolation Filters and Their Hardware Implementations,"High efficiency video coding (HEVC) fractional interpolation algorithm has very high computational complexity. In this paper, two approximate HEVC fractional interpolation filters are proposed. They significantly reduce computational complexity of HEVC fractional interpolation with a negligible PSNR loss and bit rate increase. In addition, two approximate HEVC fractional interpolation hardware are proposed. They, in the worst case, can process 45 quad full HD (3840 √ó 2160) fps. They consume up to 67.1% less energy than original HEVC fractional interpolation hardware. Therefore, they can be used in consumer electronics products that require a low energy HEVC encoder."
Energy-Aware Motion and Disparity Estimation System for 3D-HEVC with Run-Time Adaptive Memory Hierarchy,"The popularization of multimedia services has pushed forward the development of 2D/3D video-capable embedded mobile devices. Such devices require efficient energy/memory-management strategies to deal with severe memory/processing requirements and limited energy supply. Therefore, we propose a Motion and Disparity Estimation system &#x2013; the most memory/processing demanding encoding steps &#x2013; for the 3D-HEVC standard. It was designed for low-energy consumption, featuring a run-time adaptive memory hierarchy. The processing unit employs Flexible Coding Order and optimizations to reduce the computational effort by exploring the inter-channel and inter-view redundancies. The memory hierarchy features window-based prefetching, data reuse, subsampling, and dynamic voltage scaling controlled by our Depth-Based Dynamic Search Window Resizing algorithm. Memory results demonstrate an average on-chip energy reduction of 79% in comparison to the widely used Level-C solution for a 45nm technology. The proposed Energy-Aware Motion and Disparity Estimation system dissipates 7.55W while processing three HD 1080p views (video &#x002B; depth) at 30 frames per second and presents a mean energy consumption of 0.107J per Access Unit. To the best of our knowledge, this is the first work that proposes a real-time ME/DE system for the 3D-HEVC standard with an adaptive memory hierarchy."
Resource-efficient Reconfigurable Computer-on-Module for Embedded Vision Applications,"The paper proposes a novel architecture for a highly customisable FPGA-SoC-based Computer-on-Module (CoM) targeting embedded vision applications. Apart from a Xilinx Zynq SoC, the module integrates an Adapteva Epiphany floating point accelerator in a Toradex Apalis compliant form factor. The CoM has been successfully integrated into two robot platforms to enhance their vision processing capabilities. For evaluation, visually-guided collision avoidance and navigation has been implemented, mimicking the behaviour of insects. The hardware/software partitioning is presented together with a comparison to an HLS-based solution for the given application. The proposed stream-based FPGA implementation achieves a speedup of 721 and an increase in energy efficiency by a factor of 800 compared to an OpenCV-based implementation on one of the embedded ARM processors of the Zynq SoC."
Hardware-accelerated Data Acquisition and Authentication for High-speed Video Streams on Future Heterogeneous Automotive Processing Platforms,"With the increasing use of Ethernet-based communication backbones in safety-critical real-time domains, both efficient and predictable interfacing and cryptographically secure authentication of high-speed data streams are becoming very important. Although the increasing data rates of in-vehicle networks allow the integration of more demanding (e.g., camera-based) applications, processing speeds and, in particular, memory bandwidths are no longer scaling accordingly. The need for authentication, on the other hand, stems from the ongoing convergence of traditionally separated functional domains and the extended connectivity both in- (e.g., smart-phones) and outside (e.g., telemetry, cloud-based services and vehicle-to-X technologies) current vehicles. The inclusion of cryptographic measures thus requires careful interface design to meet throughput, latency, safety, security and power constraints given by the particular application domain. Over the last decades, this has forced system designers to not only optimize their software stacks accordingly, but also incrementally move interface functionalities from software to hardware. This paper discusses existing and emerging methods for dealing with high-speed data streams ranging from software-only via mixed-hardware/software approaches to fully hardware-based solutions. In particular, we introduce two approaches to acquire and authenticate GigE Vision Video Streams at full line rate of Gigabit Ethernet on Programmable SoCs suitable for future heterogeneous automotive processing platforms."
Code Improvements Towards Implementing HEVC Decoder,HEVC is the latest video coding standard achieving better compression as compared to previous standards but at a higher complexity. Many applications require fast and real-time decoding of compressed video for which Field Programmable Gate Array (FPGA) is one of the efficient options. In this paper we provide the code improvement and implementation of the computationally intensive HEVC decoder. We describe our investigation for the implementation of HEVC decoder on Visual C++ 2008 platform and present improvements to the code of HEVC decoder required prior to its conversion to a hardware description language. The results for decoding time are also provided.
"Soft-decision LCC Decoder Architecture with n=4 for RS(255,239)","In this work we present a novel architecture for a soft-decision Reed-Solomon LCC decoder. In our decoder the data sorted and stored in the Multiplicity Assignment stage are different from other authors' work. We present architectures for the Syndrome Update and Symbol Modification blocks that are adapted to the proposed sorting stage. We present implementation results for ASIC and FPGA that show that this architecture can reach high performance and low latency when compared with similar decoders. For example, in ASIC, our decoder requires 40% less area than a specific state-of-the-art decoder, while still has 40% higher throughput and 0.07 dB coding gain over that decoder."
Decoding JPEG XS on a GPU,"JPEG XS is an upcoming lightweight image compression standard that is especially developed to meet the requirements of compressed video-over-IP use cases. It is designed with not only CPU, FPGA or ASIC platforms in mind, but explicitly also targets GPUs. Though not yet finished, the codec is now sufficiently mature to present a first NVIDIA CUDA-based GPU decoder architecture and preliminary performance results. On a 2014 mid-range GPU with 640 cores a 12 bit UHD 4:2:2 (4:4:4) can be decoded with 54 (42) fps. The algorithm scales very well: on a 2017 high-end GPU with 2560 cores the throughput increases to 190 (150) fps. In contrast, an optimized GPU-accelerated JPEG 2000 decoder takes 2x as long for high compression ratios that yield a PSNR of 40 dB and 3x as long for lower compression ratios with a PSNR of over 50 dB."
An Approach to Achieve Zero Turnaround Time in TDD Operation on SDR Front-End,"Thanks to the digitization and softwarization of radio communication, the development cycle of new radio technologies can be significantly accelerated by prototyping on software-defined radio (SDR) platforms. However, a slow turnaround time (TT) of the front-end of an SDR for switching from receiving mode to transmitting mode or vice versa, are jeopardizing the prototyping of wireless protocols, standards, or systems with stringent latency requirements. In this paper, a novel solution called BaseBand processing unit operating in Half Duplex mode and analog Radio Frequency front-end operating in Full Duplex mode, BBHD-RFFD, is presented to reduce the TT on SDR. A prototype is realized on the widely adopted AD9361 radio frequency frontend to prove the validity of the proposed solution. Experiments unveil that for any type of application, the TT in time division duplex (TDD) operation mode can be reduced to zero by the BBHD-RFFD approach, with negligible impact on the communication system in terms of receiver sensitivity. The impact is measured for an in-house IEEE 802.15.4 compliant transceiver. When compared against the conventional TDD approach, only a 7.5-dB degradation is observed with the BBHD-RFFD approach. The measured sensitivity of ‚àí91 dBm is still well above the minimum level (i.e., ‚àí85 dBm at 2.4 GHz) defined by the IEEE 802.15.4 standard."
Highly Paralleled Low-Cost Embedded HEVC Video Encoder on TI KeyStone Multicore DSP,"Although HEVC, the emerging video coding standard, has doubled the coding performance of its predecessor H.264/AVC, its significantly increased computational complexity imposes great obstacles for HEVC encoders to be employed in real-time applications with embedded processors, such as digital signal processors (DSPs). In this paper, a TI Keystone multicore TMS320C6678 DSP-based highly paralleled low-cost fast HEVC encoding solution is well designed and implemented. First, the overall structure of HEVC encoder with CTU-level parallelism is re-designed to well support the encoding parallelism, with full consideration of the hardware characteristics. Second, a low-delay and low-memory multicore data transmission mechanism is proposed to reduce the latency of data access between internal L2 memory and external DDR3. Third, the encoding bottlenecks, i.e., the most time-consuming encoding modules, are identified and optimized for acceleration with TI powerful C6000 SIMD instructions. Experimental results show that our proposed HEVC encoder on TI TMS320C6678 DSPs can significantly improve the real-time capacity with tolerable performance loss, 0.93 dB performance loss under on average 465.50 times speedup as compared to CPU-based HM reference software, more specifically, which makes it desirable in power-constrained real-time video applications."
OpenCL Performance Prediction using Architecture-Independent Features,"OpenCL is an attractive programming model for heterogeneous high-performance computing systems, with wide support from hardware vendors and significant performance portability. To support efficient scheduling on HPC systems it is necessary to perform accurate performance predictions for OpenCL workloads on varied compute devices, which is challenging due to diverse computation, communication and memory access characteristics which result in varying performance between devices. The Architecture Independent Workload Characterization (AIWC) tool can be used to characterize OpenCL kernels according to a set of architecture-independent features. This work presents a methodology where AIWC features are used to form a model capable of predicting accelerator execution times. We used this methodology to predict execution times for a set of 37 computational kernels running on 15 different devices representing a broad range of CPU, GPU and MIC architectures. The predictions are highly accurate, differing from the measured experimental run-times by an average of only 1.2%, and correspond to actual execution time mispredictions of 9 ps to 1 sec according to problem size. A previously unencountered code can be instrumented once and the AIWC metrics embedded in the kernel, to allow performance prediction across the full range of modelled devices. The results suggest that this methodology supports correct selection of the most appropriate device for a previously unen- countered code, which is highly relevant to the HPC scheduling setting."
A Multi-Kernel Multi-Code Polar Decoder Architecture,"Polar codes have received increasing attention in the past decade, and have been selected for the next generation of the wireless communication standard. Most research on polar codes has focused on codes constructed from a 2√ó2 polarization matrix, called binary kernel: codes constructed from binary kernels have code lengths that are bound to powers of 2. A few recent works have proposed construction methods based on multiple kernels of different dimensions, not only binary ones, allowing code lengths different from powers of 2. In this paper, we design and implement the first multi-kernel successive cancellation polar code decoder in literature. It can decode any code constructed with binary and ternary kernels: the architecture, sized for a maximum code length N<sub>max</sub>, is fully flexible in terms of code length, code rate, and kernel sequence. The decoder can achieve a frequency of over 1 GHz in 65 nm CMOS technology, and a throughput of 615 Mb/s. The area occupation ranges between 0.11 mm<sup>2</sup>for N<sub>max</sub>= 256 and 2.01 mm<sup>2</sup>for N<sub>max</sub>= 4096. Implementation results show an unprecedented degree of flexibility: with N<sub>max</sub>= 4096, up to 55 code lengths can be decoded with the same hardware, along with any kernel sequence and code rate."
LDPC Soft Decoding with Improved Performance in 1X-2X MLC and TLC NAND Flash-Based Solid State Drives,"The reliability of non-volatile NAND flash memories is reaching critical levels for traditional error detection and correction. Therefore, to ensure data trustworthiness in nowadays NAND flash-based Solid State Drives, it is essential to exploit powerful correction algorithms such as the Low Density Parity Check. However, the burdens of this approach materialize in a disk performance reduction. In this work a standard decoding approach is compared with an optimized solution exploiting hardware resources available in NAND flash chips. The simulation results on 2X, 1X and mid-1X MLC and TLC NAND flashbased Solid State Drives in terms of disk bandwidth, average latency, and Quality of Service favor the adoption of the presented solution in different host scenarios and realistic workloads. The proposed solution is particularly effective when high error correction interventions and read- or write-intensive workloads are considered."
AligneR: A Process-in-Memory Architecture for Short Read Alignment in ReRAMs,"Genomics is the key to enable the personal customization of medical care. How to fast and energy-efficiently analyze the huge amounts of genomic sequence data generated by next generation sequencing technologies has become one of the most significant challenges facing genomics today. Existing hardware platforms achieve low genome sequencing throughput with significant hardware and power overhead. In this paper, we propose AligneR, a ReRAM-based process-in-memory architecture, to accelerate the bottleneck of genome sequencing, i.e., short read alignment. Compared to state-of-the-art accelerators, AligneR improves the short read alignment throughput per Watt per<inline-formula><tex-math notation=""LaTeX"">$mm^2$</tex-math><alternatives><inline-graphic xlink:href=""jiang-ieq1-2854700.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>by<inline-formula><tex-math notation=""LaTeX"">$13\times$</tex-math><alternatives><inline-graphic xlink:href=""jiang-ieq2-2854700.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>."
Resistive CAM Acceleration for Tunable Approximate Computing,"The Internet of Things significantly increases the amount of data generated, straining the processing capability of current computing systems. Approximate computing is a promising solution to accelerate computation by trading off energy and accuracy. In this paper, we propose a resistive content addressable memory (CAM) accelerator, called RCA, which exploits data locality to have an approximate memory-based computation. RCA stores high frequency patterns and performs computation inside CAM without using processing cores. During execution time, RCA searches an input operand among all prestored values on a CAM and returns the row with the nearest distance. To manage accuracy, we use a distance metric which considers the impact of each bit indices on computation accuracy. We evaluate an application of proposed RCA on CPU approximation, where RCA can be used as a stand-alone or as a hybrid computing unit besides CPU cores for tunable CPU approximation. We evaluate the architecture of the proposed RCA using HSPICE and multi2sim by testing our results on x86 CPU processor. Our evaluation shows that RCA can accelerate CPU computation by 12.6 and improve the energy efficiency by 6.6 as compared to a traditional CPU architecture, while providing acceptable quality of service."
Efficient Job Offloading in Heterogeneous Systems through Hardware-assisted Packet-based Dispatching and User-level Runtime Infrastructure,"Emerging heterogeneous systems architectures increasingly integrate general-purpose processors, GPUs, and other specialized computational units to provide both power and performance benefits. While the motivations for developing systems with accelerators are clear, it is important to design efficient dispatching mechanisms in terms of performance and energy while leveraging programmability and orchestration of the diverse computational components. In this article we present an infrastructure composed of a hardware, General, Packet-based Processing-dispatching Unit, named GPPU, and of an associated runtime that facilitates user-level access to GPPU objects such as packets, queues and contexts. Hence, we remove drawbacks of traditional costly user-to-kernel-level operations, low-level accelerator subtleties that hinder programming productivity, along with architectural obstacles such as handling accelerators‚Äô unified virtual address space. We present the design and evaluation of our framework by integrating the GPPU infrastructure with data streaming type accelerators, image filtering and matrix multiplication, tightly coupled to ARMv8 architecture via unified virtual memory. Under scaling workload our proposed dispatching methods can deliver 3.7√ó performance improvement over baseline offloading, and up to 4.7√ó better energy efficiency."
Selective Gray-Coded Bit-Plane-Based Two-Bit Transform and Its Efficient Hardware Architecture for Low-Complexity Motion Estimation,"In this paper, a novel low bit-depth representation-based motion estimation approach with its hardware architecture is presented. The low bit-depth representation of pixels in the proposed method is constructed by choosing them from the gray-coded bit planes in an efficient way. Additionally, a new matching creation is also presented to improve the motion estimation accuracy. Thanks to the proposed binarization approach, the computation load is considerably reduced compared to filtering-based low bit-depth representation motion estimation methods. A novel hardware architecture for the proposed method is also presented in this paper. Experimental results revealed that the proposed motion estimation method and its hardware architecture provides a good balance between motion estimation accuracy and hardware resources especially for consumer electronics applications."
A Hardware-Oriented IME Algorithm for HEVC and Its Hardware Implementation,"High Efficiency Video Coding (HEVC), the latest video coding standard, aims to provide coding performance that is much superior to that of its predecessor, H.264, especially for high definition video. To fulfill this goal, the inter-prediction unit (PU) partitions of HEVC are more complex, and the search range of motion estimation (ME) is much larger. As a result, ME becomes a bottleneck in the design of the HEVC inter predictor. In response to this challenge, we developed a hardware-oriented integer ME algorithm and the related hardware implementation. Our proposed algorithm led to a decrease in terms of the Bjontegaard Delta rate when compared with the HEVC test model 15.0. The corresponding hardware solution benefitted from 2-D data reuse supported by horizontal and vertical reference SRAMs, on-chip memory reduction supported by 4 √ó 4 block compression, and a low-power sum of absolute difference (SAD) tree supported by PU-level chip selection. When adopting a 32 √ó 32 SAD tree, the minimum and maximum required working frequency for 4K √ó 2K at 30 frames/s videos was [375, 500] MHz. These results demonstrated that our proposed solution offered desirable improvement in both coding speed and coding performance."
Modeling the Energy Consumption of the HEVC Decoding Process,"In this paper, we present a bit stream feature-based energy model that accurately estimates the energy required to decode a given High Efficiency Video Coding-coded bit stream. Therefore, we take a model from literature and extend it by explicitly modeling the in-loop filters, which was not done before. Furthermore, to prove its superior estimation performance, it is compared with seven different energy models from the literature. By using a unified evaluation framework, we show how accurately the required decoding energy for different decoding systems can be approximated. We give thorough explanations on the model parameters and explain how the model variables are derived. To show the modeling capabilities in general, we test the estimation performance for different decoding software and hardware solutions, where we find that the proposed model outperforms the models from the literature by reaching framewise mean estimation errors of less than 7% for software and less than 15% for hardware-based systems."
General-Purpose Graphics Processor Architecture,"<p>Originally developed to support video games, graphics processor units (GPUs) are now increasingly used for general-purpose (non-graphics) applications ranging from machine learning to mining of cryptographic currencies. GPUs can achieve improved performance and efficiency versus central processing units (CPUs) by dedicating a larger fraction of hardware resources to computation. In addition, their general-purpose programmability makes contemporary GPUs appealing to software developers in comparison to domain-specific accelerators. This book provides an introduction to those interested in studying the architecture of GPUs that support general-purpose computing. It collects together information currently only found among a wide range of disparate sources. The authors led development of the GPGPU-Sim simulator widely used in academic research on GPU architectures.</p> <p>The first chapter of this book describes the basic hardware structure of GPUs and provides a brief overview of their history. Chapter 2 provides a summary of GPU programming models relevant to the rest of the book. Chapter 3 explores the architecture of GPU compute cores. Chapter 4 explores the architecture of the GPU memory system. After describing the architecture of existing systems, Chapters \ref{ch03} and \ref{ch04} provide an overview of related research. Chapter 5 summarizes cross-cutting research impacting both the compute core and memory system.</p> <p>This book should provide a valuable resource for those wishing to understand the architecture of graphics processor units (GPUs) used for acceleration of general-purpose applications and to those who want to obtain an introduction to the rapidly growing body of research exploring how to improve the architecture of these GPUs.</p>"
Performance Analysis of Depth Intra Coding in 3D-HEVC,"The depth maps intra-frame prediction of 3D High-Efficiency Video Coding (3D-HEVC) inherits all texture encoding techniques provided by HEVC and provides new coding tools for depth map predictions. These tools comprise algorithms such as bipartition modes, intra-picture skip, and DC-only. This paper details these tools and shows how they work together with the original HEVC algorithms in the depth map intra-frame prediction for allowing a high efficient encoding. Besides, this paper analyzes the encoding time and the encoding mode distribution of the intra-frame prediction tools over different quantization scenarios. We aim to provide support for upcoming works on depth map encoding, including complexity reduction and control, real-time embedded systems implementations, and even the development of improved tools to encode depth maps."
Stereoview to Multiview Conversion Architecture for Auto-Stereoscopic 3D Displays,"In this paper, a stereoview to multiview conversion system, which includes stereo matching and depth image-based rendering (DIBR) hardware designs, is proposed. To achieve an efficient architecture, the proposed stereo matching algorithm simply generates the raw matching costs and aggregates cost based on 1D iterative aggregation schemes. For the DIBR architecture, an inpainting-based method is used to find the most similar patch from the background, according to depth information. The simulation results show that the designed architecture achieves an averaged peak signal-to-noise ratio of 30.2 dB and structure similarity of 0.94 for the tested images. The hardware design for the proposed 2D to 3D conversion system operates at a maximum clock frequency of 160.2 MHz for outputting 1080p (1920 √ó 1080) video at 60 frames per second."
Deep neural network acceleration framework under hardware uncertainty,"Deep Neural Networks (DNNs) are known as effective model to perform cognitive tasks. However, DNNs are computationally expensive in both train and inference modes as they require the precision of floating point operations. Although, several prior work proposed approximate hardware to accelerate DNNs inference, they have not considered the impact of training on accuracy. In this paper, we propose a general framework called FramNN, which adjusts DNN training model to make it appropriate for underlying hardware. To accelerate training FramNN applies adaptive approximation which dynamically changes the level of hardware approximation depending on the DNN error rate. We test the efficiency of the proposed design over six popular DNN applications. Our evaluation shows that in inference, our design can achieve 1.9√ó energy efficiency improvement and 1.7√ó speedup while ensuring less than 1% quality loss. Similarly, in training mode FramNN can achieve 5.0√ó energy-delay product improvement as compared to baseline AMD GPU."
CANNA: Neural network acceleration using configurable approximation on GPGPU,"Neural networks have been successfully used in many applications. Due to their computational complexity, it is difficult to implement them on embedded devices. Neural networks are inherently approximate and thus can be simplified. In this paper, CANNA proposes a gradual training approximation which adaptively sets the level of hardware approximation depending on the neural network's internal error, instead of apply uniform hardware approximation. To accelerate inference, CANNA's layer-based approximation approach selectively relaxes the computation in each layer of neural network, as a function its sensitivity to approximation. For hardware support, we use a configurable floating point unit in Hardware that dynamically identifies inputs which produce the largest approximation error and process them instead in precise mode. We evaluate the accuracy and efficiency of our design by integrating configurable FPUs into AMD's Southern Island GPU architecture. Our experimental evaluation shows that CANNA achieves up to 4.84√ó (7.13√ó) energy savings and 3.22√ó (4.64√ó) speedup when training four different neural network applications with 0% (2%) quality loss as compared to the implementation on baseline GPU. During the inference phase, our layer-based approach improves the energy efficiency by 4.42√ó (6.06√ó) and results in 2.96√ó (3.98√ó) speedup while ensuring 0% (2%) quality loss."
Low-Power HEVC 1-D IDCT Hardware Architecture,"This paper presents a low-power (High Efficiency Video Coding) HEVC 1-D IDCT (One-Dimension Inverse Discrete Cosine Transform) hardware architecture, employing a bypass engine to reduce power dissipation. The bypass engine reduces power by replacing the regular 1-D IDCT algorithm by much simpler operations when applicable. Due to an average applicability rate of 87.57%, the low-power HEVC 1-D IDCT can substantially reduce the power dissipation with a slight area overhead. ASIC synthesis results estimates the power dissipation in 3.12 mW when operating at 789.32 MHz. Such frequency is enough to real-time encoding of Ultra-High Definition (UHD 4K) videos at 60 frames per second. Moreover, the presented energy saving hardware architecture can reduce 26% in power dissipation when compared to the regular 1-D IDCT hardware architecture."
Embedded Solutions for Deep Neural Networks Implementation,"Deep Neural Networks and its associate learning paradigm-Deep Learning-represents today a breakthrough in the field of Artificial Intelligence due to the impressive results obtained in many application areas, especially in image, video or speech processing. The main hindrance to the development process of such applications is represented by the vast amount of computational power needed to train such structures. Various hardware solutions arose to this problem, most of them relying on the intrinsic parallelism found in modern Graphical Processing Units. On the other hand, once the learning process was finished, the functional phase (inference) of the neural network require substantially less hardware resources enabling thus potential realtime solutions. Our work provides an extensive overview regarding currently available embedded solutions for Deep Neural Networks implementation, pointing out the main characteristics, advantages and disadvantages. We also demonstrate through experimental results that the effect of combined hardware optimization and suitable deep architecture could substantially decrease the inference process execution time."
DeepPicar: A Low-Cost Deep Neural Network-Based Autonomous Car,"We present DeepPicar, a low-cost deep neural network based autonomous car platform. DeepPicar is a small scale replication of a real self-driving car called DAVE-2 by NVIDIA. DAVE-2 uses a deep convolutional neural network (CNN), which takes images from a front-facing camera as input and produces car steering angles as output. DeepPicar uses the same network architecture-9 layers, 27 million connections and 250K parameters-and can drive itself in real-time using a web camera and a Raspberry Pi 3 quad-core platform. Using DeepPicar, we analyze the Pi 3's computing capabilities to support end-to-end deep learning based real-time control of autonomous vehicles. We also systematically compare other contemporary embedded computing platforms using the DeepPicar's CNN-based real-time control workload. We find that all tested platforms, including the Pi 3, are capable of supporting the CNN-based real-time control, from 20 Hz up to 100 Hz, depending on hardware platform. However, we find that shared resource contention remains an important issue that must be considered in applying CNN models on shared memory based embedded computing platforms; we observe up to 11.6X execution time increase in the CNN based control loop due to shared resource contention. To protect the CNN workload, we also evaluate state-of-the-art cache partitioning and memory bandwidth throttling techniques on the Pi 3. We find that cache partitioning is ineffective, while memory bandwidth throttling is an effective solution."
Optimisation of HEVC motion estimation exploiting SAD and SSD GPU-based implementation,"The new High-Efficiency Video Coding (HEVC) standard doubles the video compression ratio compared to the previous H.264/AVC at the same video quality and without any degradation. However, this important performance is achieved by increasing the encoder computational complexity. That's why HEVC complexity is a crucial subject. The most time consuming and the most intensive computing part of HEVC is the motion estimation based principally on the sum of absolute differences (SAD) or the sum of square differences (SSD) algorithms. For these reasons, the authors proposed an implementation of these algorithms on a low cost NVIDIA GPU (graphics processing unit) using the Fermi architecture developed with Compute Unified Device Architecture language. The proposed algorithm is based on the parallel-difference and the parallel-reduction process. The investigational results show a significant speed-up in terms of execution time for most 64 √ó 64 pixel blocks. In fact, the proposed parallel algorithm permits a significant reduction in the execution time that reaches up to 56.17 and 30.4%, compared to the CPU, for SAD and SSD algorithms, respectively. This improvement proves that parallelising the algorithm with the new proposed reduction process for the Fermi-GPU generation leads to better results. These findings are based on a static study that determines the PU percentage utilisation for each dimension in the HEVC. This study shows that the larger PUs are the most utilised in temporal levels 3 and 4, which attain 84.56% for class E. This improvement is accompanied by an average peak signal-to-noise ratio loss of 0.095 dB and a decrease of 0.64% in terms of BitRate."
